// Step 1: Extract cntrprty_client_lkpid from pos_frame and collect into a List[String]
val cntrprtyClientList = pos_frame
  .select("cntrprty_client_lkpid")
  .as[String]
  .collect()
  .toList

// Step 2: Filter the rdr_client_full_name based on cntrprtyClientList
val filteredRdrClientFullName = rdr_client_full_name
  .filter(col("clt_oid").isin(cntrprtyClientList: _*))

// Step 3a: Group by clt_credit_ult_prnt and collect distinct values
val creditUltPrntDF = filteredRdrClientFullName
  .groupBy("clt_credit_ult_prnt")
  .agg(collect_list("clt_oid").as("distinct_clt_oid_credit"))

// Step 3b: Group by clt_prf_ult_prnt and collect distinct values
val prfUltPrntDF = filteredRdrClientFullName
  .groupBy("clt_prf_ult_prnt")
  .agg(collect_list("clt_oid").as("distinct_clt_oid_profit"))

// Step 3c: Group by clt_oid and collect distinct values
val oidDF = filteredRdrClientFullName
  .groupBy("clt_oid")
  .agg(collect_list("clt_oid").as("distinct_clt_oid"))

// Step 4: Merge the three lists (distinct_clt_oid_credit, distinct_clt_oid_profit, distinct_clt_oid) into a single list
import org.apache.spark.sql.functions._

val mergedListDF = creditUltPrntDF
  .select(explode(col("distinct_clt_oid_credit")).as("clt_oid"))
  .union(prfUltPrntDF.select(explode(col("distinct_clt_oid_profit")).as("clt_oid")))
  .union(oidDF.select(explode(col("distinct_clt_oid")).as("clt_oid")))
  .distinct() // Ensure distinct values in the merged list

// Collect the merged distinct list into a List[String]
val mergedClientOidList = mergedListDF
  .select("clt_oid")
  .as[String]
  .collect()
  .toList

// Step 5: Use the merged list to filter rdr_client_full_name
val finalRdrClientFullName = rdr_client_full_name
  .filter(col("clt_oid").isin(mergedClientOidList: _*))

// Show the filtered results
finalRdrClientFullName.show(truncate = false)

public static Dataset<Row> unionAllSummary(Dataset<Row> firstDataset, Dataset<Row>... additionalDatasets) {
    log.info("Util.unionAllSummary: START");

    // Initialize orderedColumns and unionedDatasets
    String[] orderedColumns = new String[0];
    Dataset<Row> unionedDatasets = null;

    // Check if the first dataset is not null
    if (firstDataset != null) {
        orderedColumns = firstDataset.columns();
        unionedDatasets = firstDataset;
    }

    // Loop through the additional datasets
    for (Dataset<Row> additionalDataset : additionalDatasets) {
        if (additionalDataset != null) {
            // Align columns in case the datasets do not have the same columns
            Dataset<Row> alignedDataset = alignColumns(additionalDataset, orderedColumns);

            if (unionedDatasets == null) {
                // First time assigning unionedDatasets when additionalDataset is not null
                orderedColumns = alignedDataset.columns();
                unionedDatasets = alignedDataset;
            } else {
                // Union the datasets
                unionedDatasets = unionedDatasets.union(alignedDataset);
            }
        }
    }

    log.info("Util.unionAllSummary: END");
    return unionedDatasets;
}

/**
 * Align the columns of a dataset to match the specified orderedColumns.
 * If a column is missing, it will be added with null values.
 *
 * @param dataset         The dataset that needs alignment
 * @param requiredColumns The columns to align to
 * @return                Aligned dataset
 */
private static Dataset<Row> alignColumns(Dataset<Row> dataset, String[] requiredColumns) {
    List<Column> selectExpr = new ArrayList<>();

    for (String colName : requiredColumns) {
        if (Arrays.asList(dataset.columns()).contains(colName)) {
            selectExpr.add(functions.col(colName));
        } else {
            selectExpr.add(functions.lit(null).alias(colName));
        }
    }

    // Select the aligned columns from the dataset
    return dataset.select(JavaConverters.asScalaBufferConverter(selectExpr).asScala().toSeq());
}

public static Dataset<Row> unionAllSummary(List<Dataset<Row>> datasets, String[] columns) {
    log.info("Util.unionAllSummary: START");
    
    String[] orderedColumns = columns;
    log.info("Using columns --> {}", Arrays.toString(orderedColumns));
    
    Dataset<Row> unionedDatasets = null;
    
    for (Dataset<Row> additionalDataset : datasets) {
        if (additionalDataset != null) {
            try {
                if (unionedDatasets == null) {
                    unionedDatasets = additionalDataset.select(orderedColumns);
                } else {
                    unionedDatasets = unionedDatasets.union(
                        additionalDataset.select(orderedColumns)
                    );
                }
                
                log.info("Current schema after union --> {}", unionedDatasets.schema());
                unionedDatasets.show(1, false);  // Show 1 row without truncation
            } catch(Exception ex) {
                log.error("Error during union operation: " + ex.getMessage());
                ex.printStackTrace();
            }
        }
    }
    
    log.info("Util.unionAllSummary: END");
    if (unionedDatasets != null) {
        unionedDatasets.show(1, false);  // Show final result
    } else {
        log.warn("No valid datasets were processed.");
    }
    
    return unionedDatasets;
}
